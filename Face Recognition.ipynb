{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Prototype\n",
    "\n",
    "This purpose of this notebook to experiment with Face Recognition. In particular the method use is that described in the Facenet paper by Florian Schroff, Dmitry Kalenichenko, and James Philbin (https://arxiv.org/abs/1503.03832).\n",
    "\n",
    "I leverage some great work that has been done by David Sandberg who has shared an implementation of facenet here (https://github.com/davidsandberg/facenet). To run this notebook, clone this repository and adjust the system path to the location of facenet/src.\n",
    "\n",
    "To perform the database search, I utilize the FAISS library from Facebook (https://github.com/facebookresearch/faiss). This repository should also be cloned. It is a C++ library with Python wrapper. Both must be build before it can be used.\n",
    "\n",
    "This project also depends on Tensorflow. Installation instructions are here: https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewatts/Library/Python/2.7/lib/python/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Checkout facenet to same root directory as this repository.\n",
    "sys.path.append(\"../facenet/src\")\n",
    "import facenet\n",
    "import align.detect_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Faiss\n",
    "\n",
    "1. Clone https://github.com/facebookresearch/faiss\n",
    "2. brew install llvm\n",
    "3. copy one of the makefiles in faiss/example_makefiles (I chose makefile.inc.Mac.brew) and rename it to makefile.inc.\n",
    "4. In IndexScalarQuantizer.cpp add the following:\n",
    "```c++\n",
    "  #ifndef __clang__\n",
    "  #include <malloc.h>\n",
    "  #endif\n",
    "```\n",
    "\n",
    "4. Then \"make\" to build C++ library.\n",
    "6. Then \"make py\" to build python wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../faiss\")\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding database\n",
    "Currently using the Labeled Faces in the Wild data set to experiment around with. This comprises about 13,000 photos of 6500 different identities. Generating the embedding for this is very time consuming (approx 16 hours on macbook pro), so this has been done and results saved in csv file to reload easily.  \n",
    "\n",
    "To generate embeddings on a different dataset run \"Build Face Database\" notebook. \n",
    "\n",
    "The code below loads from csv file and loads up the Faiss index. Results from Faiss index return the index of the embedding. To look up the name find the same index in the face_identities array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"faces_lfw.csv\")\n",
    "\n",
    "face_identities = []\n",
    "face_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    identity = row['id']\n",
    "    embedding = row.iloc[1:129].as_matrix().astype('float32')\n",
    "    embedding = np.ascontiguousarray(embedding.reshape(1, 128))\n",
    "    face_index.add(embedding)\n",
    "    face_identities.append(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Face Detection and Embedding Neural Networks.\n",
    "\n",
    "1. Face Detection:\n",
    "Face detection is done by feeding an image through a MTCNN (Multi task Convolutional Neural Network).  This process is described in this paper by Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao (https://arxiv.org/abs/1604.02878).\n",
    "\n",
    "This implementation comes from David Sandberg facenet.\n",
    "\n",
    "\n",
    "2. Face Embedding:\n",
    "Once a face has been detected, the face is fed through another neural network to generate the face embedding.  An embedding is a 128 float vector which can be compared with embeddings from other faces.   Embeddings that are simliar (i.e. Eucleadean space) represent the same person.\n",
    "\n",
    "This model is an Inception ResNet v1 architecture. This Neural Network has been trained on the MS-Celeb-1M data set using triplet loss.\n",
    "\n",
    "Before running this cell, download the model at https://drive.google.com/open?id=1uFRZtwdJbu1ND_y1Abj6okCtdjhLig6M and unzip in a models subdirectory.\n",
    "\n",
    "Implementation Detail:\n",
    "Both Neural Networks are loaded into the same tensorflow graph to greatly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../facenet/src/align/detect_face.py:210: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From ../facenet/src/align/detect_face.py:212: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = \"./20170512-110547/20170512-110547.pb\"\n",
    "\n",
    "facenet_graph = tf.Graph()\n",
    "with facenet_graph.as_default():\n",
    "    facenet_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(MODEL_FILE, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        facenet_graph_def.ParseFromString(serialized_graph)            \n",
    "        tf.import_graph_def(facenet_graph_def, name='enet')\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            enet = lambda img : sess.run(('enet/embeddings:0'), feed_dict={'enet/input:0':img, 'enet/phase_train:0':False})\n",
    "            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Face Detection constants.\n",
    "MIN_FACE_SIZE = 20                     # minimum size of the face for the MTCNN\n",
    "DETECT_THRESHOLDS = [ 0.6, 0.7, 0.7 ]  # threshold values for the three stages of the MTCNN\n",
    "SCALE_FACTOR = 0.709                   # MTCNN scale factor\n",
    "\n",
    "# Face Embedding constants.\n",
    "INPUT_IMAGE_SIZE = 160\n",
    "\n",
    "# This function normalizes the image before generating the embedding.\n",
    "def run_facenet(image):\n",
    "    image_data = np.around(image/255.0, decimals=12)\n",
    "    image_data = np.expand_dims(image_data, axis=0)\n",
    "    return enet(image_data)\n",
    "\n",
    "def register_face(image, name):\n",
    "    # Remove alpha.\n",
    "    image = image[:,:,0:3]\n",
    "    \n",
    "    # get image dimensions for later calculations.\n",
    "    height, width = image.shape[0:2]\n",
    "    \n",
    "    # Find bounding boxes for all faces.  Ignore facial landmarks for now.\n",
    "    bb, landmarks = align.detect_face.detect_face(image, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "\n",
    "    faces = bb.shape[0]\n",
    "    boxes = np.zeros((faces, 4), dtype=np.int32)\n",
    "    \n",
    "    if (faces == 1):\n",
    "        boxes[0, 0] = np.maximum(bb[0, 0], 0)\n",
    "        boxes[0, 1] = np.maximum(bb[0, 1], 0)\n",
    "        boxes[0, 2] = np.minimum(bb[0, 2], width)\n",
    "        boxes[0, 3] = np.minimum(bb[0, 3], height)\n",
    "\n",
    "        # Crop and scale for input into embedding neural network (160x160)\n",
    "        cropped = image[boxes[0, 1]:boxes[0, 3],boxes[0, 0]:boxes[0, 2],:]\n",
    "        scaled = misc.imresize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interp='bilinear')\n",
    "        \n",
    "        embedding = run_facenet(scaled)\n",
    "        \n",
    "        face_index.add(embedding)\n",
    "        face_identities.append(name)\n",
    "        return \"Registered.\", scaled \n",
    "    else:\n",
    "        return \"More than one face detected: \" + str(faces), None\n",
    "\n",
    "def identify_faces_in_image(image):   \n",
    "    # Remove alpha.\n",
    "    image = image[:,:,0:3]\n",
    "    \n",
    "    # get image dimensions for later calculations.\n",
    "    height, width = image.shape[0:2]\n",
    "    \n",
    "    # Find bounding boxes for all faces.\n",
    "    # Note, flip image colors as this is what detect_face expects.\n",
    "    bb, landmarks = align.detect_face.detect_face(image, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "\n",
    "    faces = bb.shape[0]\n",
    "    \n",
    "    # Allocate the bounding boxes and embeddings for each face.\n",
    "    boxes = np.zeros((faces, 4), dtype=np.int32)\n",
    "    embeddings = np.zeros((faces, 128), dtype=np.float32)\n",
    "    \n",
    "    for faceIx in range(0, faces):    \n",
    "        # Make sure box is in size of image.\n",
    "        boxes[faceIx, 0] = np.maximum(bb[faceIx, 0], 0)\n",
    "        boxes[faceIx, 1] = np.maximum(bb[faceIx, 1], 0)\n",
    "        boxes[faceIx, 2] = np.minimum(bb[faceIx, 2], width)\n",
    "        boxes[faceIx, 3] = np.minimum(bb[faceIx, 3], height)\n",
    "\n",
    "        # Crop and scale for input into embedding neural network (160x160)\n",
    "        cropped = image[boxes[faceIx, 1]:boxes[faceIx, 3],boxes[faceIx, 0]:boxes[faceIx, 2],:]\n",
    "        scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Generate the embedding.\n",
    "        embeddings[faceIx,:] = run_facenet(scaled)\n",
    "\n",
    "    # Search through face_index to find the indicies into the identities array, and the distance.\n",
    "    distances, indicies = face_index.search(embeddings, 2)\n",
    "    \n",
    "    return boxes, landmarks, indicies, distances, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython@5/5.5.0_2/libexec/vendor/lib/python2.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered.\n",
      "Registered.\n",
      "More than one face detected: 2\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "name = \"Jeff Watts\"\n",
    "\n",
    "vc = cv2.VideoCapture(0)\n",
    "register_count = 10\n",
    "\n",
    "while register_count>0:\n",
    "    time.sleep(.5)\n",
    "    is_capturing, frame = vc.read()\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    message, registered_face = register_face(image, name)\n",
    "    print(message)\n",
    "    if (registered_face is not None):\n",
    "        cv2.imwrite(\"register\" + str(register_count) + \".png\",registered_face)\n",
    "    register_count -= 1\n",
    "    \n",
    "vc.release()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    is_capturing, frame = vc.read()\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, landmarks, indicies, distances, embeddings = identify_faces_in_image(image)\n",
    "\n",
    "    faces = boxes.shape[0]\n",
    "\n",
    "    for faceIx in range(0, faces):\n",
    "        identity = face_identities[indicies[faceIx,0]]\n",
    "        distance = distances[faceIx, 0]\n",
    "        distance_next = distances[faceIx, 1]\n",
    "        if (distance > 0.6):\n",
    "            identity = \"Unknown\"\n",
    "\n",
    "        xmin = boxes[faceIx,0]\n",
    "        ymin = boxes[faceIx,1]\n",
    "        xmax = boxes[faceIx,2]\n",
    "        ymax = boxes[faceIx,3]\n",
    "\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 165, 20), 4)\n",
    "        \n",
    "        text = identity + \": \" + str(distance) + \"/\" + str(distance_next)\n",
    "        cv2.putText(frame, text, (xmin, ymin-15),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 165, 20), 2)\n",
    "        \n",
    "        faceIx_landmarks = landmarks[:,faceIx]\n",
    "        \n",
    "        for landmarkIx in range(0, faceIx_landmarks.shape[0]/2):\n",
    "            x = faceIx_landmarks[landmarkIx]\n",
    "            y = faceIx_landmarks[landmarkIx+5]\n",
    "            cv2.circle(frame, (x,y), 2, (255, 165, 20), thickness=2) \n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
