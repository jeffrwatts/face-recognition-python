{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Prototype\n",
    "\n",
    "This purpose of this notebook to experiment with Face Recognition. In particular the method use is that described in the Facenet paper by Florian Schroff, Dmitry Kalenichenko, and James Philbin (https://arxiv.org/abs/1503.03832).\n",
    "\n",
    "I leverage some great work that has been done by David Sandberg who has shared an implementation of facenet here (https://github.com/davidsandberg/facenet). To run this notebook, clone this repository and adjust the system path to the location of facenet/src.\n",
    "\n",
    "To perform the database search, I utilize the FAISS library from Facebook (https://github.com/facebookresearch/faiss). This repository should also be cloned. It is a C++ library with Python wrapper. Both must be build before it can be used.\n",
    "\n",
    "This project also depends on Tensorflow. Installation instructions are here: https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Checkout facenet to same root directory as this repository.\n",
    "sys.path.append(\"../facenet/src\")\n",
    "import facenet\n",
    "import align.detect_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Faiss\n",
    "\n",
    "1. Clone https://github.com/facebookresearch/faiss\n",
    "2. brew install llvm\n",
    "3. copy one of the makefiles in faiss/example_makefiles (I chose makefile.inc.Mac.brew) and rename it to makefile.inc.\n",
    "4. In IndexScalarQuantizer.cpp add the following:\n",
    "```c++\n",
    "  #ifndef __clang__\n",
    "  #include <malloc.h>\n",
    "  #endif\n",
    "```\n",
    "\n",
    "4. Then \"make\" to build C++ library.\n",
    "6. Then \"make py\" to build python wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../faiss\")\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding database\n",
    "Currently using the Labeled Faces in the Wild data set to experiment around with. This comprises about 13,000 photos of 6500 different identities. Generating the embedding for this is very time consuming (approx 16 hours on macbook pro), so this has been done and results saved in csv file to reload easily.  \n",
    "\n",
    "To generate embeddings on a different dataset run \"Build Face Database\" notebook. \n",
    "\n",
    "The code below loads from csv file and loads up the Faiss index. Results from Faiss index return the index of the embedding. To look up the name find the same index in the face_identities array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"faces_cv2.csv\")\n",
    "\n",
    "face_identities = []\n",
    "face_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    identity = row['id']\n",
    "    embedding = row.iloc[1:129].as_matrix().astype('float32')\n",
    "    embedding = np.ascontiguousarray(embedding.reshape(1, 128))\n",
    "    face_index.add(embedding)\n",
    "    face_identities.append(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Face Detection and Embedding Neural Networks.\n",
    "\n",
    "1. Face Detection:\n",
    "Face detection is done by feeding an image through a MTCNN (Multi task Convolutional Neural Network).  This process is described in this paper by Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao (https://arxiv.org/abs/1604.02878).\n",
    "\n",
    "This implementation comes from David Sandberg facenet.\n",
    "\n",
    "\n",
    "2. Face Embedding:\n",
    "Once a face has been detected, the face is fed through another neural network to generate the face embedding.  An embedding is a 128 float vector which can be compared with embeddings from other faces.   Embeddings that are simliar (i.e. Eucleadean space) represent the same person.\n",
    "\n",
    "This model is an Inception ResNet v1 architecture. This Neural Network has been trained on the MS-Celeb-1M data set using triplet loss.\n",
    "\n",
    "Before running this cell, download the model at https://drive.google.com/open?id=1uFRZtwdJbu1ND_y1Abj6okCtdjhLig6M and unzip in a models subdirectory.\n",
    "\n",
    "Implementation Detail:\n",
    "Both Neural Networks are loaded into the same tensorflow graph to greatly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = \"./20170512-110547/20170512-110547.pb\"\n",
    "\n",
    "facenet_graph = tf.Graph()\n",
    "with facenet_graph.as_default():\n",
    "    facenet_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(MODEL_FILE, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        facenet_graph_def.ParseFromString(serialized_graph)            \n",
    "        tf.import_graph_def(facenet_graph_def, name='enet')\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            enet = lambda img : sess.run(('enet/embeddings:0'), feed_dict={'enet/input:0':img, 'enet/phase_train:0':False})\n",
    "            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Detection constants.\n",
    "MIN_FACE_SIZE = 20                     # minimum size of the face for the MTCNN\n",
    "DETECT_THRESHOLDS = [ 0.6, 0.7, 0.7 ]  # threshold values for the three stages of the MTCNN\n",
    "SCALE_FACTOR = 0.709                   # MTCNN scale factor\n",
    "\n",
    "# Face Embedding constants.\n",
    "INPUT_IMAGE_SIZE = 160\n",
    "\n",
    "# This function normalizes the image before generating the embedding.\n",
    "def run_facenet(image):\n",
    "    image_data = np.around(image/255.0, decimals=12)\n",
    "    image_data = np.expand_dims(image_data, axis=0)\n",
    "    return enet(image_data)\n",
    "\n",
    "def register_face(image, name):\n",
    "    # Remove alpha.\n",
    "    image = image[:,:,0:3]\n",
    "    \n",
    "    # get image dimensions for later calculations.\n",
    "    height, width = image.shape[0:2]\n",
    "    \n",
    "    # Find bounding boxes for all faces.  Ignore facial landmarks for now.\n",
    "    bb, landmarks = align.detect_face.detect_face(image, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "\n",
    "    faces = bb.shape[0]\n",
    "    boxes = np.zeros((faces, 4), dtype=np.int32)\n",
    "    \n",
    "    if (faces == 1):\n",
    "        boxes[0, 0] = np.maximum(bb[0, 0], 0)\n",
    "        boxes[0, 1] = np.maximum(bb[0, 1], 0)\n",
    "        boxes[0, 2] = np.minimum(bb[0, 2], width)\n",
    "        boxes[0, 3] = np.minimum(bb[0, 3], height)\n",
    "\n",
    "        # Crop and scale for input into embedding neural network (160x160)\n",
    "        cropped = image[boxes[0, 1]:boxes[0, 3],boxes[0, 0]:boxes[0, 2],:]\n",
    "        scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_LINEAR) \n",
    "        \n",
    "        embedding = run_facenet(scaled)\n",
    "        \n",
    "        face_index.add(embedding)\n",
    "        face_identities.append(name)\n",
    "        return \"Registered.\", scaled \n",
    "    else:\n",
    "        return \"More than one face detected: \" + str(faces), None\n",
    "\n",
    "def identify_faces_in_image(image):   \n",
    "    # Remove alpha.\n",
    "    image = image[:,:,0:3]\n",
    "    \n",
    "    # get image dimensions for later calculations.\n",
    "    height, width = image.shape[0:2]\n",
    "    \n",
    "    # Find bounding boxes for all faces.\n",
    "    # Note, flip image colors as this is what detect_face expects.\n",
    "    bb, landmarks = align.detect_face.detect_face(image, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "\n",
    "    faces = bb.shape[0]\n",
    "    \n",
    "    # Allocate the bounding boxes and embeddings for each face.\n",
    "    boxes = np.zeros((faces, 4), dtype=np.int32)\n",
    "    embeddings = np.zeros((faces, 128), dtype=np.float32)\n",
    "    \n",
    "    for faceIx in range(0, faces):    \n",
    "        # Make sure box is in size of image.\n",
    "        boxes[faceIx, 0] = np.maximum(bb[faceIx, 0], 0)\n",
    "        boxes[faceIx, 1] = np.maximum(bb[faceIx, 1], 0)\n",
    "        boxes[faceIx, 2] = np.minimum(bb[faceIx, 2], width)\n",
    "        boxes[faceIx, 3] = np.minimum(bb[faceIx, 3], height)\n",
    "\n",
    "        # Crop and scale for input into embedding neural network (160x160)\n",
    "        cropped = image[boxes[faceIx, 1]:boxes[faceIx, 3],boxes[faceIx, 0]:boxes[faceIx, 2],:]\n",
    "        scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_LINEAR) \n",
    "        \n",
    "        # Generate the embedding.\n",
    "        embeddings[faceIx,:] = run_facenet(scaled)\n",
    "\n",
    "    # Search through face_index to find the indicies into the identities array, and the distance.\n",
    "    distances, indicies = face_index.search(embeddings, 1)\n",
    "    \n",
    "    return boxes, landmarks, indicies, distances, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Registered.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "name = \"Jeff Watts\"\n",
    "\n",
    "vc = cv2.VideoCapture(0)\n",
    "register_count = 10\n",
    "\n",
    "while register_count>0:\n",
    "    time.sleep(.5)\n",
    "    is_capturing, frame = vc.read()\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    message, registered_face = register_face(image, name)\n",
    "    print(message)\n",
    "    if (registered_face is not None):\n",
    "        registered_face = cv2.cvtColor(registered_face, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(\"register\" + str(register_count) + \".png\", registered_face)\n",
    "    register_count -= 1\n",
    "    \n",
    "vc.release()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6ef210568b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#image = frame[...,::-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_faces_in_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3d3de90d51a7>\u001b[0m in \u001b[0;36midentify_faces_in_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Find bounding boxes for all faces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Note, flip image colors as this is what detect_face expects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mbb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_FACE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDETECT_THRESHOLDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCALE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/git/facenet/src/align/detect_face.py\u001b[0m in \u001b[0;36mdetect_face\u001b[0;34m(img, minsize, pnet, rnet, onet, threshold, factor)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mtempimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtempimg\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m127.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0078125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mtempimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0mout0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/git/facenet/src/align/detect_face.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mpnet_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pnet/conv4-2/BiasAdd:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pnet/prob1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pnet/input:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mrnet_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnet/conv5-2/conv5-2:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rnet/prob1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'rnet/input:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0monet_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'onet/conv6-2/conv6-2:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onet/conv6-3/conv6-3:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onet/prob1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'onet/input:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpnet_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnet_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monet_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    is_capturing, frame = vc.read()\n",
    "    #image = frame[...,::-1]\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, landmarks, indicies, distances, embeddings = identify_faces_in_image(image)\n",
    "\n",
    "    faces = boxes.shape[0]\n",
    "\n",
    "    for faceIx in range(0, faces):\n",
    "        identity = face_identities[indicies[faceIx,0]]\n",
    "        distance = distances[faceIx, 0]\n",
    "        if (distance > 0.6):\n",
    "            identity = \"Unknown\"\n",
    "\n",
    "        xmin = boxes[faceIx,0]\n",
    "        ymin = boxes[faceIx,1]\n",
    "        xmax = boxes[faceIx,2]\n",
    "        ymax = boxes[faceIx,3]\n",
    "\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 165, 20), 4)\n",
    "        \n",
    "        text = identity + \": \" + str(distance) \n",
    "        cv2.putText(frame, text, (xmin, ymin-15),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 165, 20), 2)\n",
    "        \n",
    "        faceIx_landmarks = landmarks[:,faceIx]\n",
    "        \n",
    "        for landmarkIx in range(0, int(faceIx_landmarks.shape[0]/2)):\n",
    "            x = faceIx_landmarks[landmarkIx]\n",
    "            y = faceIx_landmarks[landmarkIx+5]\n",
    "            cv2.circle(frame, (x,y), 2, (255, 165, 20), thickness=2) \n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
