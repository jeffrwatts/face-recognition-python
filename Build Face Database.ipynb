{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Face Database\n",
    "\n",
    "Prepare the database for faces to recognize.\n",
    "\n",
    "Download a data set under a ./data directory.  \n",
    "\n",
    "This example uses Labeled Faces in the Wild (http://vis-www.cs.umass.edu/lfw/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jewatts/Library/Python/2.7/lib/python/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Checkout facenet to same root directory as this repository.\n",
    "sys.path.append(\"../facenet/src\")\n",
    "import facenet\n",
    "import align.detect_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../facenet/src/align/detect_face.py:210: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From ../facenet/src/align/detect_face.py:212: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = \"./20170512-110547/20170512-110547.pb\"\n",
    "\n",
    "facenet_graph = tf.Graph()\n",
    "with facenet_graph.as_default():\n",
    "    facenet_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(MODEL_FILE, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        facenet_graph_def.ParseFromString(serialized_graph)            \n",
    "        tf.import_graph_def(facenet_graph_def, name='enet')\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            enet = lambda img : sess.run(('enet/embeddings:0'), feed_dict={'enet/input:0':img, 'enet/phase_train:0':False})\n",
    "            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Face Detection constants.\n",
    "MIN_FACE_SIZE = 20                     # minimum size of the face for the MTCNN\n",
    "DETECT_THRESHOLDS = [ 0.6, 0.7, 0.7 ]  # threshold values for the three stages of the MTCNN\n",
    "SCALE_FACTOR = 0.709                   # MTCNN scale factor\n",
    "\n",
    "# Face Embedding constants.\n",
    "INPUT_IMAGE_SIZE = 160\n",
    "\n",
    "# This function normalizes the image before generating the embedding.\n",
    "def run_facenet(image):\n",
    "    image_data = np.around(image/255.0, decimals=12)\n",
    "    image_data = np.expand_dims(image_data, axis=0)\n",
    "    return enet(image_data)\n",
    "\n",
    "def import_dataset(input_dir, output_dir):\n",
    "    df = None\n",
    "\n",
    "    # Pick up where we left off if we had to kill the process as it was loading.\n",
    "    if os.path.exists(\"faces.csv\"):\n",
    "        df = pd.read_csv(\"faces.csv\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    dataset = facenet.get_dataset(input_dir)\n",
    "    \n",
    "    for cls in dataset:\n",
    "        output_class_dir = os.path.join(output_dir, cls.name)\n",
    "        \n",
    "        if not os.path.exists(output_class_dir):\n",
    "            os.makedirs(output_class_dir)\n",
    "\n",
    "        for image_path in cls.image_paths:\n",
    "            filename = os.path.splitext(os.path.split(image_path)[1])[0]\n",
    "            output_filename = os.path.join(output_class_dir, filename+'.png')\n",
    "            \n",
    "            # Print what name we are on to give some idea of progress.\n",
    "            print(cls.name)\n",
    "            \n",
    "            if os.path.exists(output_filename):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                image = misc.imread(image_path)\n",
    "            except (IOError, ValueError, IndexError) as e:\n",
    "                errorMessage = '{}: {}'.format(image_path, e)\n",
    "                print(errorMessage)\n",
    "            else:\n",
    "                if image.ndim<3:\n",
    "                    print('Unable to align \"%s\"' % image_path)\n",
    "                    continue\n",
    "                    \n",
    "                # remove alpha\n",
    "                image = image[:,:,0:3]\n",
    "\n",
    "                height, width = image.shape[0:2]\n",
    "                \n",
    "                bb, _ = align.detect_face.detect_face(image, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "\n",
    "                faces = bb.shape[0]\n",
    "                \n",
    "                if (faces == 1):  \n",
    "                    box = np.zeros(4, dtype=np.int32)\n",
    "                    box[0] = np.maximum(bb[0, 0], 0)\n",
    "                    box[1] = np.maximum(bb[0, 1], 0)\n",
    "                    box[2] = np.minimum(bb[0, 2], width)\n",
    "                    box[3] = np.minimum(bb[0, 3], height)\n",
    "                    \n",
    "                    cropped = image[box[1]:box[3],box[0]:box[2],:]\n",
    "\n",
    "                    scaled = misc.imresize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interp='bilinear')\n",
    "                    embedding = run_facenet(scaled)\n",
    "                                    \n",
    "                    df1 = pd.DataFrame([cls.name], columns=[\"id\"])\n",
    "                    df2 = pd.DataFrame(embedding)\n",
    "                    row = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "                    if (df is None):\n",
    "                        df = row\n",
    "                    else:\n",
    "                        df = df.append(row)\n",
    "\n",
    "                    df.to_csv(\"faces.csv\", index=False)\n",
    "\n",
    "                    filename_base, file_extension = os.path.splitext(output_filename)\n",
    "                    output_filename_n = \"{}{}\".format(filename_base, file_extension)\n",
    "                    misc.imsave(output_filename_n, scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../faiss\")\n",
    "import faiss \n",
    "\n",
    "df_test = pd.read_csv(\"faces_test.csv\")\n",
    "\n",
    "face_identities = []\n",
    "face_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    identity = row['id']\n",
    "    embedding = row.iloc[1:129].as_matrix().astype('float32')\n",
    "    embedding = np.ascontiguousarray(embedding.reshape(1, 128))\n",
    "    face_index.add(embedding)\n",
    "    face_identities.append(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    errorMessage = ''\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if (img is None):\n",
    "        errorMessage = '{}: failed to load'.format(image_path)\n",
    "        return None, errorMessage\n",
    "    \n",
    "    height, width, channels = img.shape \n",
    "    \n",
    "    if (channels < 3):\n",
    "        errorMessage = '{}: less than three dimensions'.format(image_path)\n",
    "        return None, errorMessage\n",
    "    \n",
    "    # Remove Alpha\n",
    "    img = img[:,:,0:3]\n",
    "    \n",
    "    # convert to RGB.\n",
    "    img = img[...,::-1]        \n",
    "        \n",
    "    return img, errorMessage\n",
    "\n",
    "\n",
    "\n",
    "def import_dataset2(input_dir, output_dir, log_filename):   \n",
    "    df = None\n",
    "\n",
    "    # Pick up where we left off if we had to kill the process as it was loading.\n",
    "    if os.path.exists(\"faces.csv\"):\n",
    "        df = pd.read_csv(\"faces.csv\")\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    dataset = facenet.get_dataset(input_dir)\n",
    "    \n",
    "    log_file = open(os.path.join(output_dir, log_filename), \"w\")\n",
    "    \n",
    "    images_total = 0\n",
    "    successfully_aligned = 0\n",
    "    \n",
    "    for cls in dataset:\n",
    "        output_class_dir = os.path.join(output_dir, cls.name)\n",
    "        \n",
    "        if not os.path.exists(output_class_dir):\n",
    "            os.makedirs(output_class_dir)\n",
    "        \n",
    "        for image_path in cls.image_paths:\n",
    "            images_total += 1\n",
    "            filename = os.path.splitext(os.path.split(image_path)[1])[0]\n",
    "            output_filename = os.path.join(output_class_dir, filename+'.png')\n",
    "            \n",
    "            print(image_path)\n",
    "            \n",
    "            if not os.path.exists(output_filename):\n",
    "                img, errorMessage = load_image(image_path)\n",
    "\n",
    "                if (img is None):\n",
    "                    print(errorMessage)\n",
    "                    log_file.write(errorMessage)\n",
    "                    continue\n",
    "\n",
    "                bounding_boxes, _ = align.detect_face.detect_face(img, MIN_FACE_SIZE, pnet, rnet, onet, DETECT_THRESHOLDS, SCALE_FACTOR)\n",
    "                \n",
    "                faces = bounding_boxes.shape[0]\n",
    "                \n",
    "                if faces>0:\n",
    "                    det = bounding_boxes[:,0:4]\n",
    "                    det_arr = []\n",
    "                    img_size = np.asarray(img.shape)[0:2]\n",
    "                    if faces>1:\n",
    "                        bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n",
    "                        img_center = img_size / 2\n",
    "                        offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n",
    "                        offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n",
    "                        index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n",
    "                        det_arr.append(det[index,:])\n",
    "                    else:\n",
    "                        det_arr.append(np.squeeze(det))\n",
    "\n",
    "                    for i, det in enumerate(det_arr):\n",
    "                        det = np.squeeze(det)\n",
    "                        bb = np.zeros(4, dtype=np.int32)\n",
    "                        bb[0] = np.maximum(det[0], 0)\n",
    "                        bb[1] = np.maximum(det[1], 0)\n",
    "                        bb[2] = np.minimum(det[2], img_size[1])\n",
    "                        bb[3] = np.minimum(det[3], img_size[0])\n",
    "                        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "                        scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_LINEAR) \n",
    "                        embedding = run_facenet(scaled)\n",
    "\n",
    "                        df1 = pd.DataFrame([cls.name], columns=[\"id\"])\n",
    "                        df2 = pd.DataFrame(embedding)\n",
    "                        row = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "                        if (df is None):\n",
    "                            df = row\n",
    "                        else:\n",
    "                            df = df.append(row)\n",
    "\n",
    "                        df.to_csv(\"faces.csv\", index=False)\n",
    "\n",
    "                        successfully_aligned += 1\n",
    "                        filename_base, file_extension = os.path.splitext(output_filename)                                \n",
    "                        output_filename_n = \"{}{}\".format(filename_base, file_extension)\n",
    "                        cv2.imwrite(output_filename_n,scaled[...,::-1])\n",
    "                else:\n",
    "                    errorMessage = '{}: no faces'.format(image_path)\n",
    "                    print(errorMessage)\n",
    "                    log_file.write(errorMessage)\n",
    "    \n",
    "    print('Total number of images: %d' % images_total)\n",
    "    print('Number of successfully aligned images: %d' % successfully_aligned)\n",
    "    log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/lfw/AJ_Cook/AJ_Cook_0001.jpg\n",
      "./data/lfw/AJ_Lamas/AJ_Lamas_0001.jpg\n",
      "./data/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\n",
      "./data/lfw/Aaron_Eckhart/Aaron_Eckhart_0002.jpg\n",
      "./data/lfw/Aaron_Eckhart/Aaron_Eckhart_0002.jpg: failed to load\n",
      "./data/lfw/Aaron_Guiel/Aaron_Guiel_0001.jpg\n",
      "./data/lfw/Aaron_Patterson/Aaron_Patterson_0001.jpg\n",
      "./data/lfw/Aaron_Peirsol/Aaron_Peirsol_0001.jpg\n",
      "./data/lfw/Aaron_Peirsol/Aaron_Peirsol_0002.jpg\n",
      "./data/lfw/Aaron_Peirsol/Aaron_Peirsol_0003.jpg\n",
      "./data/lfw/Aaron_Peirsol/Aaron_Peirsol_0004.jpg\n",
      "./data/lfw/Aaron_Pena/Aaron_Pena_0001.jpg\n",
      "./data/lfw/Aaron_Sorkin/Aaron_Sorkin_0001.jpg\n",
      "./data/lfw/Aaron_Sorkin/Aaron_Sorkin_0002.jpg\n",
      "./data/lfw/Aaron_Tippin/Aaron_Tippin_0001.jpg\n",
      "./data/lfw/Abba_Eban/Abba_Eban_0001.jpg\n",
      "./data/lfw/Abbas_Kiarostami/Abbas_Kiarostami_0001.jpg\n",
      "./data/lfw/Abdel_Aziz_Al-Hakim/Abdel_Aziz_Al-Hakim_0001.jpg\n",
      "./data/lfw/Abdel_Madi_Shabneh/Abdel_Madi_Shabneh_0001.jpg\n",
      "./data/lfw/Abdel_Nasser_Assidi/Abdel_Nasser_Assidi_0001.jpg\n",
      "./data/lfw/Abdel_Nasser_Assidi/Abdel_Nasser_Assidi_0002.jpg\n",
      "./data/lfw/Abdoulaye_Wade/Abdoulaye_Wade_0001.jpg\n",
      "./data/lfw/Abdoulaye_Wade/Abdoulaye_Wade_0002.jpg\n",
      "./data/lfw/Abdoulaye_Wade/Abdoulaye_Wade_0003.jpg\n",
      "./data/lfw/Abdoulaye_Wade/Abdoulaye_Wade_0004.jpg\n",
      "./data/lfw/Abdul_Majeed_Shobokshi/Abdul_Majeed_Shobokshi_0001.jpg\n",
      "./data/lfw/Abdul_Rahman/Abdul_Rahman_0001.jpg\n",
      "./data/lfw/Abdulaziz_Kamilov/Abdulaziz_Kamilov_0001.jpg\n",
      "./data/lfw/Abdullah/Abdullah_0001.jpg\n",
      "./data/lfw/Abdullah/Abdullah_0002.jpg\n",
      "./data/lfw/Abdullah/Abdullah_0003.jpg\n",
      "./data/lfw/Abdullah/Abdullah_0004.jpg\n",
      "./data/lfw/Abdullah_Ahmad_Badawi/Abdullah_Ahmad_Badawi_0001.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0001.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0002.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0003.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0004.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0005.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0006.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0007.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0008.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0009.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0010.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0011.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0012.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0013.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0014.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0015.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0016.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0017.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0018.jpg\n",
      "./data/lfw/Abdullah_Gul/Abdullah_Gul_0019.jpg\n",
      "Total number of images: 51\n",
      "Number of successfully aligned images: 50\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./data/lfw\"\n",
    "output_dir = \"./data/lfw-test-output\"\n",
    "import_dataset2(input_dir, output_dir, \"logfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 11]\n",
      " [11 10]\n",
      " [12 32]\n",
      " [13 85]\n",
      " [14 91]\n",
      " [15 27]\n",
      " [16  5]\n",
      " [17 18]\n",
      " [18 17]\n",
      " [19 22]\n",
      " [20 22]\n",
      " [21 92]\n",
      " [22 20]\n",
      " [23 45]\n",
      " [24 29]]\n",
      "[[6.3201631e-05 7.1960950e-01]\n",
      " [5.4959943e-05 7.2201103e-01]\n",
      " [5.8500544e-05 1.0370395e+00]\n",
      " [6.6950524e-05 1.3488963e+00]\n",
      " [6.1983919e-05 1.2586166e+00]\n",
      " [1.2931286e-04 9.5449734e-01]\n",
      " [8.4476116e-05 1.2845230e+00]\n",
      " [4.1195308e-05 1.4957476e-01]\n",
      " [4.7756308e-05 1.4767630e-01]\n",
      " [5.9083090e-05 4.8276648e-01]\n",
      " [5.6836077e-05 2.9722852e-01]\n",
      " [5.1570346e-04 5.0957000e-01]\n",
      " [5.0823026e-05 2.9537436e-01]\n",
      " [3.6490237e-05 1.0283061e+00]\n",
      " [5.9899612e-05 1.1914573e+00]]\n",
      "Aaron_Sorkin\n",
      "Aaron_Sorkin\n",
      "Aaron_Tippin\n",
      "Abba_Eban\n",
      "Abbas_Kiarostami\n",
      "Abdel_Aziz_Al-Hakim\n",
      "Abdel_Madi_Shabneh\n",
      "Abdel_Nasser_Assidi\n",
      "Abdel_Nasser_Assidi\n",
      "Abdoulaye_Wade\n",
      "Abdoulaye_Wade\n",
      "Abdoulaye_Wade\n",
      "Abdoulaye_Wade\n",
      "Abdul_Majeed_Shobokshi\n",
      "Abdul_Rahman\n"
     ]
    }
   ],
   "source": [
    "OFFSET=10\n",
    "TEST_FACES = 15\n",
    "\n",
    "df_test = pd.read_csv(\"faces.csv\")\n",
    "\n",
    "embeddings = np.zeros((TEST_FACES, 128), dtype=np.float32)\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    adjusted_index = index - OFFSET\n",
    "    \n",
    "    if (adjusted_index == TEST_FACES):\n",
    "        break\n",
    "        \n",
    "    if (adjusted_index >= 0):\n",
    "        embedding = row.iloc[1:129].as_matrix().astype('float32')\n",
    "        embeddings[adjusted_index,:] = np.ascontiguousarray(embedding.reshape(1, 128))\n",
    "    \n",
    "\n",
    "distances, indicies = face_index.search(embeddings, 2)\n",
    "print(indicies)\n",
    "print(distances)\n",
    "\n",
    "for ix in range(len(indicies)):\n",
    "    print(face_identities[indicies[ix,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
